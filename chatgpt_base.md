2023年3月1日，openai官方发布了新的chatGPT的api接口。老接口的模型是`text-davinci-003`，新接口的模型是`gpt-3.5-turbo`。新接口的相应速度比原来快了很多，并且 **`支持上下文`**。 

# model

## gpt-3.5-turbo
```python
import openai
openai.ChatCompletion.create(
  model="gpt-3.5-turbo",
  messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Who won the world series in 2020?"},
        {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
        {"role": "user", "content": "Where was it played?"}
    ]
)
```
消息包括system(系统指令)、user(用户发言)、assistant(机器回复)这三部分，其中最重要的是系统指令，用于初始化ChatGPT的行为，也就是规定ChatGPT后续的行为模式。

- **system系统指令**: 有助于设置assistant机器回复的行为，比如指定后续用什么角色回复用户的提问。在下面的例子中，assistant被指示为“我是一个和尚...”。
- **user用户发言**:  通常是用户输入，描述用户要问的内容，也就是prompt。
- **assistant机器回复**: 由于模型对过去的请求没有记忆，默认是不能理解上下文的。 如果用 assistant 存储先前的响应，这样下一次用户再提问时,引用了以前的消息时,后台可以自动帮助openai理解上下文。如果对话超过令牌数量的限制，则需要以某种方式缩短它。
  - 使用 **gpt-3.5-turbo**，您必须保留要传递给模型的对话上下文，以生成相关的连贯响应。这就是它的设计方式。如果您的请求超过 **4096** 个标记，您甚至可能需要通过语义搜索或其他方式总结、修剪或提取相关部分来缩短它，但这本身就是一个完整的主题。

#### 维持会话状态上下文
我们调用的ChatGPT的API是无状态的，意味着你需要自己去维持会话状态，保存上下文，每次请求的时候将之前的历史消息全部发过去，但是这里面有两个问题：

1. 为了建立持续会话，请求内容会越来越大，这些内容前后的关联关系并不是很大；

2. 语言模型以称为 tokens 的块读取文本，需要为为每个 token 支付费用，这样Token 费用很高。

3. 为了保证API调用的有效性，令牌总数必须是 低于模型的最大限制（gpt-3.5-turbo-0301 为 4096 个令牌）

我们借助OpenAI的embedding模型(**特征向量**)和自己的数据库，**先在本地搜索数据获得上下文，然后在调用ChatGPT的API的时候，加上本地数据库中的相关内容**，这样就可以让ChatGPT从你自己的数据集获得了上下文 ，再结合ChatGPT自己庞大的数据集给出一个更相关的理想结果。

**大致的构建过程**：

1. 获取某一特定领域里大量的带有答案的中文或者英文问题（本文将之称为标准问题集），把它变成CSV或者Json这样易于处理的格式，并且分成小块（chunks），每块不要超过8191个Tokens，因为这是OpenAI embeddings模型的输入长度限制。
2. 使用OpenAI的embedding模型将这些问题转化为特征向量，需要将转换后的结果保存到本地数据库。 注意一般的关系型数据库是不支持这种向量数据的，需要**使用向量数据库**，这里使用Milvus，同时Milvus将给这些特征向量分配一个向量ID。
3. 当然你保存的时候，可以把原始的文本块和数字向量一起存储，这样可以根据数字向量反向获得原始文本。也可以将这些代表问题的ID和其对应的答案存储在SQL关系数据库中, 这一步有点类似于全文索引中给数据建索引。

## embedding 
嵌入embedding用来计算文本或者字符串的**特征向量**，可以用来衡量文本字符串的相关性，两个向量之间的距离衡量它们的相关性。小距离表示高相关性，大距离表示低相关性。距离函数的选择通常无关紧要，但我们推荐**余弦相似度cosine similarity**。 目前主要使用的模型是： **`text-embedding-ada-002`**。

嵌入embedding被归一化为长度 1，这意味着：
- 仅使用点积可以稍微更快地计算余弦相似度
- 余弦相似度和欧几里德距离将导致相同的排名

#### 例子： 通过语义相似度匹配来实现一个问答系统
大致的构建过程：

1. 获取某一特定领域里大量的带有答案的中文或者英文问题（本文将之称为标准问题集），把它变成CSV或者Json这样易于处理的格式，并且分成小块（chunks），**每块不要超过8191个Tokens**，因为这是OpenAI embeddings模型的输入长度限制。
2. 使用OpenAI的embedding模型将这些问题转化为特征向量，需要将转换后的结果保存到本地数据库。 注意一般的关系型数据库是不支持这种向量数据的，需要使用向量数据库，这里使用 [Milvus](https://github.com/milvus-io/milvus) ，同时Milvus将给这些特征向量分配一个向量ID。
3. 当然你保存的时候，可以把原始的文本块和数字向量一起存储，这样可以根据数字向量反向获得原始文本。也可以将这些代表问题的ID和其对应的答案存储在SQL关系数据库SQL 中, 这一步有点类似于全文索引中给数据建索引。
   
当用户提出一个问题时：
1. 通过OpenAI的embedding模型将之转化为特征向量
2. 在Milvus中对特征向量做相似度检索，得到与该问题最相似的标准问题的id, 拿到这个数字向量后，再去自己的数据库进行检索，那么就可以得到一个结果集，这个结果集会根3据匹配的相似度有个打分，分越高说明越匹配， 这样就可以按照匹配度倒序返回一个相关结果。
3. 在SQL数据库得出对应的结果集。然后根据拿到的结果集，将结果集加入到请求ChatGPT的prompt中。
